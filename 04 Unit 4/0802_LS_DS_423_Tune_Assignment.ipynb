{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0802 LS_DS_423_Tune_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.22.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGGrt9EYlCqY"
      },
      "source": [
        "\n",
        "\n",
        "# Tune Practice\n",
        "\n",
        "## *Data Science Unit 4 Sprint 2 Assignment 3*\n",
        "\n",
        "# Gridsearch Hyperparameters\n",
        "\n",
        "In the guided project, you learned how to use sklearn's GridsearchCV and keras-tuner library to tune the hyperparameters of a neural network model. For your module project, you'll continue using these two libraries; however, we will make things a little more interesting for you. \n",
        "\n",
        "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. \n",
        "\n",
        "\n",
        "\n",
        "**Don't forget to switch to GPU on Colab!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvn8EcRmMpxE",
        "outputId": "54a308c1-2c43-40f6-bb6f-92c4e7e345e9"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 29.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 97 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.41.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.6.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.0.4 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lzoonloI0Ot"
      },
      "source": [
        "# native python libraries imports \n",
        "import math\n",
        "from time import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# sklearn imports \n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# keras imports \n",
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "from kerastuner.tuners import RandomSearch, BayesianOptimization, Sklearn\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from keras.activations import relu, sigmoid\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.utils import get_file\n",
        "\n",
        "# required for compatibility between sklearn and keras\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxXxCMJpI0Ou"
      },
      "source": [
        "def load_quickdraw10():\n",
        "    \"\"\"\n",
        "    Fill out this docstring, and comment the code for practice in writing the kind of code that will get you hired. \n",
        "    \"\"\"\n",
        "    \n",
        "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
        "    \n",
        "    path_to_zip = get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
        "\n",
        "    data = np.load(path_to_zip)\n",
        "    \n",
        "    # normalize your image data\n",
        "    max_pixel_value = 255\n",
        "    X = data['arr_0']/max_pixel_value\n",
        "    Y = data['arr_1']\n",
        "        \n",
        "    return train_test_split(X, Y, shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45m5XaCFI0Ou",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda2af98-aa0b-439b-b58e-c612a9fdca29"
      },
      "source": [
        "X_train, X_test, y_train, y_test = load_quickdraw10()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\n",
            "25427968/25421363 [==============================] - 0s 0us/step\n",
            "25436160/25421363 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf11zxcCI0Ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab56b482-32ed-45dd-c62a-5ff9a28ce08b"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CoZCMoII0Ov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8dc4fc-7610-40f1-c7d2-abffc6697052"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(75000,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pugKBJQ2I0Ow"
      },
      "source": [
        "_____\n",
        "\n",
        "# Experiment 1\n",
        "\n",
        "## Tune Hyperparameters Using Enhanced GridsearchCV \n",
        "\n",
        "We will use GridsearchCV again to tune a deep learning model; however, we will add some additional functionality to our gridsearch. Specifically, we will automate away the generation of how many nodes to use in a layer and how many layers to use in a model!\n",
        "\n",
        "By the way, yes, there is a function within a function. Try not to let that bother you. An alternative to this would be to create a class. If you're up for the challenge, give it a shot. However, consider this a stretch goal that you come back to after going through this assignment. \n",
        "\n",
        "\n",
        "### Objective \n",
        "\n",
        "This experiment aims to show you how to automate the generation of layers and layer nodes for gridsearch. Up until now, we've been manually selecting the number of layers and layer nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXjs7Hk71Hy"
      },
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(n_layers,  first_layer_nodes, last_layer_nodes, act_funct =\"relu\", negative_node_incrementation=True):\n",
        "    \"\"\"\"\n",
        "    Returns a complied keras model \n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    n_layers: int \n",
        "        number of hidden layers in the model \n",
        "        To be clear, this excludes the input and output layers.\n",
        "        \n",
        "    first_layer_nodes: int\n",
        "        Number of nodes in the first hidden layer \n",
        "\n",
        "    last_layer_nodes: int\n",
        "        Number of nodes in the last hidden layer (this is the layer before the output layer)\n",
        "        \n",
        "     act_funct: string \n",
        "         Name of activation function to use in hidden layers (this excludes the output layer)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    model: keras object \n",
        "    \"\"\"\n",
        "    \n",
        "    def gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation=True):\n",
        "        \"\"\"\n",
        "        Generates and returns the number of nodes in each hidden layer. \n",
        "        To be clear, this excludes the input and output layers. \n",
        "\n",
        "        Note\n",
        "        ----\n",
        "        The number of nodes in each layer is linearly incremented. \n",
        "        For example, gen_layer_nodes(5, 500, 100) will generate [500, 400, 300, 200, 100]\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_layers: int\n",
        "            The number of hidden layers\n",
        "            These values should be 2 or greater \n",
        "\n",
        "        first_layer_nodes: int\n",
        "\n",
        "        last_layer_nodes: int\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        layers: list of ints\n",
        "            Contains the number of nodes for each layer \n",
        "        \"\"\"\n",
        "\n",
        "        # throws an error if n_layers is less than 2 \n",
        "        assert n_layers >= 2, \"n_layers must be 2 or greater\"\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # PROTIP: IF YOU WANT THE NODE INCREMENTATION TO BE SPACED DIFFERENTLY\n",
        "        # THEN YOU'LL NEED TO CHANGE THE WAY THAT IT'S CALCULATED - HAVE FUN!\n",
        "        # when set to True number of nodes, are decreased for subsequent layers \n",
        "        if negative_node_incrementation:\n",
        "            # subtract this amount from the previous layer's nodes to increment towards smaller numbers \n",
        "            nodes_increment = (last_layer_nodes - first_layer_nodes)/ (n_layers-1)\n",
        "            \n",
        "        # when set to False number of nodes are increased for subsequent layers\n",
        "        else:\n",
        "            # add this amount from previous layer's nodes in order to increment towards larger numbers \n",
        "            nodes_increment = (first_layer_nodes - last_layer_nodes)/ (n_layers-1)\n",
        "\n",
        "        nodes = first_layer_nodes\n",
        "\n",
        "        for i in range(1, n_layers+1):\n",
        "\n",
        "            layers.append(math.ceil(nodes))\n",
        "\n",
        "            # increment nodes for next layer \n",
        "            nodes = nodes + nodes_increment\n",
        "\n",
        "        return layers\n",
        "    \n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    \n",
        "    n_nodes = gen_layer_nodes(n_layers, first_layer_nodes, last_layer_nodes, negative_node_incrementation)\n",
        "    \n",
        "    for i in range(1, n_layers):\n",
        "        if i==1:\n",
        "            model.add(Dense(first_layer_nodes, input_dim=X_train.shape[1], activation=act_funct))\n",
        "        else:\n",
        "            model.add(Dense(n_nodes[i-1], activation=act_funct))\n",
        "            \n",
        "            \n",
        "    # output layer \n",
        "    model.add(Dense(10, # 10 unit/neurons in output layer because we have 10 possible labels to predict  \n",
        "                    activation='softmax')) # use softmax for a label set greater than 2            \n",
        "    \n",
        "    # Compile model\n",
        "    model.compile(loss='sparse_categorical_crossentropy', \n",
        "                  optimizer='adam', # adam is a good default optimizer \n",
        "                  metrics=['accuracy'])\n",
        "    \n",
        "    # do not include model.fit() inside the create_model function\n",
        "    # KerasClassifier is expecting a compiled model \n",
        "    return model\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLJcMMvDI0Oz"
      },
      "source": [
        "## Explore Create_Model\n",
        "\n",
        "Let's build a few different models to understand how the above code works in practice. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2MXQbEjI0Oz"
      },
      "source": [
        "### Build Model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5dcf5c585f07629a03086cf57ba53615",
          "grade": false,
          "grade_id": "cell-86d63e89a21223de",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "gHH9RRINI0O0"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model = create_model(10, 500, 100)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwUyuxOiI0O0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f598122-aebb-420a-bd46-e99e517e564e"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes has been linearly incremented in decreasing values. \n",
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 456)               228456    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 412)               188284    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 367)               151571    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 323)               118864    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 278)               90072     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 234)               65286     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 189)               44415     \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 145)               27550     \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1460      \n",
            "=================================================================\n",
            "Total params: 1,308,458\n",
            "Trainable params: 1,308,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqcE5zEI0O0"
      },
      "source": [
        "### Build Model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 10` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = False`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e0722533c325d699f4842e874e43720e",
          "grade": false,
          "grade_id": "cell-99d563a291231a7b",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "xtjvYSjCI0O1"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "model2 = create_model(10, 500, 100, negative_node_incrementation=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-A5IJJUI0O1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d17b5253-c6bd-4bc4-874c-074052adb7fb"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "# Notice in the model summary how the number of nodes has been linearly incremented in increasing values.\n",
        "# The output layer must have 10 nodes because there are 10 labels to predict \n",
        "model2.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_220 (Dense)            (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_221 (Dense)            (None, 545)               273045    \n",
            "_________________________________________________________________\n",
            "dense_222 (Dense)            (None, 589)               321594    \n",
            "_________________________________________________________________\n",
            "dense_223 (Dense)            (None, 634)               374060    \n",
            "_________________________________________________________________\n",
            "dense_224 (Dense)            (None, 678)               430530    \n",
            "_________________________________________________________________\n",
            "dense_225 (Dense)            (None, 723)               490917    \n",
            "_________________________________________________________________\n",
            "dense_226 (Dense)            (None, 767)               555308    \n",
            "_________________________________________________________________\n",
            "dense_227 (Dense)            (None, 812)               623616    \n",
            "_________________________________________________________________\n",
            "dense_228 (Dense)            (None, 856)               695928    \n",
            "_________________________________________________________________\n",
            "dense_229 (Dense)            (None, 10)                8570      \n",
            "=================================================================\n",
            "Total params: 4,166,068\n",
            "Trainable params: 4,166,068\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXCjnun6I0O1"
      },
      "source": [
        "# feel free to play around with parameters to gain additional insight as to how the create_model function works \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxaShY_-I0O2"
      },
      "source": [
        "Ok, now that we've played around a bit with  `create_model` to understand how it works, let's build a much simpler model that we'll be running gridsearches. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9oHYqblI0O2"
      },
      "source": [
        "### Build Model \n",
        "\n",
        "Use `create_model` to build a model. \n",
        "\n",
        "- Set `n_layers = 2` \n",
        "- Set `first_layer_nodes = 500`\n",
        "- Set `last_layer_nodes = 100`\n",
        "- Set `act_funct = \"relu\"`\n",
        "- Make sure that `negative_node_incrementation = True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "606b85d0ba4531836f97caf6850297f8",
          "grade": false,
          "grade_id": "cell-4ca6c5e51302fd10",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "5UpVuVk2I0O2"
      },
      "source": [
        "# use create_model to create a model \n",
        "\n",
        "# YOUR CODE HERE\n",
        "gridmod = create_model(2, 500, 100)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjNMxvG4I0O2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3696e38e-7c12-47d9-ccef-a47324ce99f9"
      },
      "source": [
        "# run model.summary() and make sure that you understand the model architecture that you just built \n",
        "gridmod.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_230 (Dense)            (None, 500)               392500    \n",
            "_________________________________________________________________\n",
            "dense_231 (Dense)            (None, 10)                5010      \n",
            "=================================================================\n",
            "Total params: 397,510\n",
            "Trainable params: 397,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1XFAIRsI0O2"
      },
      "source": [
        "# define the grid search parameters\n",
        "param_grid = {'n_layers': [2, 3],\n",
        "              'epochs': [3], \n",
        "              \"first_layer_nodes\": [500, 300],\n",
        "              \"last_layer_nodes\": [100, 50]\n",
        "             }"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EJCj9myI0O3"
      },
      "source": [
        "keramodel = KerasClassifier(create_model)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGJhDcxFI0O3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df2e97ad-0be1-48f0-b93c-ae311b92b1fd"
      },
      "source": [
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=keramodel, \n",
        "                    param_grid=param_grid, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6609 - accuracy: 0.8020\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4470 - accuracy: 0.8663\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3582 - accuracy: 0.8934\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4728 - accuracy: 0.8644\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6630 - accuracy: 0.8023\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4479 - accuracy: 0.8667\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.3584 - accuracy: 0.8925\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4586 - accuracy: 0.8651\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6695 - accuracy: 0.7997\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4525 - accuracy: 0.8656\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3626 - accuracy: 0.8921\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4501 - accuracy: 0.8693\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6340 - accuracy: 0.8062\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4313 - accuracy: 0.8688\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3433 - accuracy: 0.8951\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4568 - accuracy: 0.8665\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6353 - accuracy: 0.8066\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4301 - accuracy: 0.8694\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3402 - accuracy: 0.8949\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4654 - accuracy: 0.8630\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6380 - accuracy: 0.8033\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4346 - accuracy: 0.8668\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3472 - accuracy: 0.8930\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4468 - accuracy: 0.8678\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6620 - accuracy: 0.8025\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.4479 - accuracy: 0.8658\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3586 - accuracy: 0.8928\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4669 - accuracy: 0.8652\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6661 - accuracy: 0.8006\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4516 - accuracy: 0.8646\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3639 - accuracy: 0.8898\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4592 - accuracy: 0.8653\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6666 - accuracy: 0.7994\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4496 - accuracy: 0.8654\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3624 - accuracy: 0.8906\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4468 - accuracy: 0.8718\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6360 - accuracy: 0.8060\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4340 - accuracy: 0.8679\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3459 - accuracy: 0.8928\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.4511 - accuracy: 0.8670\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6405 - accuracy: 0.8055\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4328 - accuracy: 0.8671\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3418 - accuracy: 0.8952\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4607 - accuracy: 0.8664\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.6386 - accuracy: 0.8030\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.4346 - accuracy: 0.8677\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 9s 6ms/step - loss: 0.3464 - accuracy: 0.8931\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4349 - accuracy: 0.8717\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6821 - accuracy: 0.7972\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4667 - accuracy: 0.8607\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3831 - accuracy: 0.8863\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4921 - accuracy: 0.8571\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6906 - accuracy: 0.7936\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4726 - accuracy: 0.8588\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3862 - accuracy: 0.8849\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4687 - accuracy: 0.8629\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6917 - accuracy: 0.7941\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4774 - accuracy: 0.8575\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3936 - accuracy: 0.8837\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4831 - accuracy: 0.8612\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6520 - accuracy: 0.8030\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4471 - accuracy: 0.8656\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3588 - accuracy: 0.8900\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4741 - accuracy: 0.8635\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6562 - accuracy: 0.8015\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4472 - accuracy: 0.8628\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3683 - accuracy: 0.8858\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4506 - accuracy: 0.8666\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6614 - accuracy: 0.7992\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4492 - accuracy: 0.8627\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3679 - accuracy: 0.8875\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4489 - accuracy: 0.8665\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6850 - accuracy: 0.7978\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4682 - accuracy: 0.8608\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3839 - accuracy: 0.8856\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4908 - accuracy: 0.8592\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6904 - accuracy: 0.7955\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4767 - accuracy: 0.8594\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3905 - accuracy: 0.8837\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4691 - accuracy: 0.8630\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6872 - accuracy: 0.7937\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.4683 - accuracy: 0.8598\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.3838 - accuracy: 0.8853\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4735 - accuracy: 0.8618\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6534 - accuracy: 0.8009\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.4451 - accuracy: 0.8644\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.3602 - accuracy: 0.8894\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4786 - accuracy: 0.8610\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6569 - accuracy: 0.8002\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4483 - accuracy: 0.8633\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3614 - accuracy: 0.8904\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4603 - accuracy: 0.8602\n",
            "Epoch 1/3\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6597 - accuracy: 0.7990\n",
            "Epoch 2/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.4486 - accuracy: 0.8654\n",
            "Epoch 3/3\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.3659 - accuracy: 0.8877\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.4557 - accuracy: 0.8668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-2)]: Done  24 out of  24 | elapsed:  9.1min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: 0.5914 - accuracy: 0.8203\n",
            "Epoch 2/3\n",
            "2344/2344 [==============================] - 13s 6ms/step - loss: 0.4062 - accuracy: 0.8768\n",
            "Epoch 3/3\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: 0.3308 - accuracy: 0.8979\n",
            "Best: 0.868399997552236 using {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8662399848302206, Stdev: 0.002169606671081633 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8657866517702738, Stdev: 0.002019998904801311 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8674266537030538, Stdev: 0.003092599080820445 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.868399997552236, Stdev: 0.0023603511023760405 with: {'epochs': 3, 'first_layer_nodes': 500, 'last_layer_nodes': 50, 'n_layers': 3}\n",
            "Means: 0.8603866497675577, Stdev: 0.0024366847574931786 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 2}\n",
            "Means: 0.8655199805895487, Stdev: 0.0014433354156144567 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 100, 'n_layers': 3}\n",
            "Means: 0.8613066673278809, Stdev: 0.0015900725914306205 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 2}\n",
            "Means: 0.8626400033632914, Stdev: 0.0029297647325983696 with: {'epochs': 3, 'first_layer_nodes': 300, 'last_layer_nodes': 50, 'n_layers': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Urhe6JWmI0O4"
      },
      "source": [
        "best_model = grid_result.best_estimator_"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wO0TkEJ0I0O4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101746c5-a268-4d61-f68d-f9bd0d0b1ff9"
      },
      "source": [
        "best_model.get_params()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'build_fn': <function __main__.create_model>,\n",
              " 'epochs': 3,\n",
              " 'first_layer_nodes': 500,\n",
              " 'last_layer_nodes': 50,\n",
              " 'n_layers': 3}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cydDGYdYI0O4"
      },
      "source": [
        "-----\n",
        "\n",
        "# Experiment 2\n",
        "\n",
        "## Benchmark Different Optimization Algorithms \n",
        "\n",
        "In this section, we are going to use the same model and dataset to benchmark 3 different gridsearch approaches: \n",
        "\n",
        "- Random Search\n",
        "- Bayesian Optimization\n",
        "- Brute Force Gridsearch\n",
        "\n",
        "Our goal in this experiment is two-fold. We want to see which approach: \n",
        "\n",
        "- Scores the highest accuracy\n",
        "- Has the shortest run time \n",
        "\n",
        "We want to see how these 3 gridsearch approaches handle these trade-offs and give you a sense of those trade-offs.\n",
        "\n",
        "### Trade-Offs\n",
        "\n",
        "`Brute Force Gridsearch` will train a model on every unique hyperparameter combination; this guarantees that you'll get the highest possible accuracy from your parameter set, but your gridsearch might have a very long run-time.\n",
        "\n",
        "`Random Search` will randomly sample from your parameter set, which, depending on how many samples, the run-time might be significantly cut down. Still, you might or might not sample the parameters that correspond to the highest possible accuracies.\n",
        "\n",
        "`Bayesian Optimization` has a bit of intelligence built into its search algorithm, but you must manually select some parameters that greatly influence the model learning outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oreSpsCWI0O4"
      },
      "source": [
        "-------\n",
        "### Build our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drNaKnp8I0O4"
      },
      "source": [
        "# because gridsearching can take a lot of time, and we are bench-marking 3 different approaches\n",
        "# let's build a simple model to minimize run time \n",
        "\n",
        "def build_model(hp):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a compiled keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units=hp.get('units'),activation=hp.get(\"activation\")))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(hp.get('learning_rate')),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "  "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLpWzs86I0O5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "0bb4b092-4f5a-4147-ec16-8fc88ca3d9d2"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hp = HyperParameters()\n",
        "hp.Int('units', min_value=32, max_value=512, step=32)\n",
        "hp.Choice('learning_rate',values=[1e-1, 1e-2, 1e-3])\n",
        "hp.Choice('activation',values=[\"relu\", \"sigmoid\"])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'relu'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJN2yATmI0O5"
      },
      "source": [
        "------\n",
        "# Run the Gridsearch Algorithms \n",
        "\n",
        "### Random Search\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `RandomSearch` tuner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aaff9aae33845f374e15f2381719d83a",
          "grade": false,
          "grade_id": "cell-8c1dfb9b6d12bea2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "I6fWPOeyI0O5"
      },
      "source": [
        "# how many unique hyperparameter combinations do we have? \n",
        "# HINT: take the product of the number of possible values for each hyperparameter \n",
        "# save your answer to n_unique_hparam_combos\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_unique_hparam_combos = 90\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9d628451e83431e1b52da10eccf2c00",
          "grade": false,
          "grade_id": "cell-1fa83950bb2d5f92",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-c7AUuMkI0O5"
      },
      "source": [
        "# how many of these do we want to randomly sample?\n",
        "# let's pick 25% of n_unique_hparam_combos param combos to sample\n",
        "# save this number to n_param_combos_to_sample\n",
        "\n",
        "# YOUR CODE HERE\n",
        "n_param_combos_to_sample = 22"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB05gTlII0O5"
      },
      "source": [
        "random_tuner = RandomSearch(\n",
        "            build_model,\n",
        "            objective='val_accuracy',\n",
        "            max_trials=n_param_combos_to_sample, # number of times to sample the parameter set and build a model \n",
        "            seed=1234,\n",
        "            hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "            directory='./keras-tuner-trial',\n",
        "            project_name='random_search')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQ0Cf2KI0O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e1811b-110d-4f75-d399-1351fe75ae5d"
      },
      "source": [
        "# take note of Total elapsed time in print out\n",
        "random_tuner.search(X_train, y_train,\n",
        "                    epochs=3,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 22 Complete [00h 00m 23s]\n",
            "val_accuracy: 0.35040000081062317\n",
            "\n",
            "Best val_accuracy So Far: 0.8715199828147888\n",
            "Total elapsed time: 00h 11m 02s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f80fCuyfI0O5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9733fb-b5b3-47e6-b4b2-efecd95c3959"
      },
      "source": [
        "# identify the best score and hyperparameter (should be at the top since scores are ranked)\n",
        "random_tuner.results_summary(num_trials=1)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/random_search\n",
            "Showing 1 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 352\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8715199828147888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "2-22HMaYI0O6"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the best performing hyperparameter combination and model score. Note that because this is a random search, multiple runs might have slightly different outcomes.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f084b5d373f8589a1de8d6d4473b974a",
          "grade": true,
          "grade_id": "cell-5527738b6382c164",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "uVtC1cIQI0O6"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "* units: 352\n",
        "* learning_rate: 0.001\n",
        "* activation: relu\n",
        "\n",
        "Score: 87.15% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5LxscSjI0O6"
      },
      "source": [
        "------\n",
        "### Bayesian Optimization\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/0/02/GpParBayesAnimationSmall.gif)\n",
        "\n",
        "Be sure to check out the [**docs for Keras-Tuner**](https://keras-team.github.io/keras-tuner/documentation/tuners/). Here you can read about the input parameters for the `BayesianOptimization` tuner.\n",
        "\n",
        "Pay special attention to these `BayesianOptimization` parameters: `num_initial_points` and `beta`. \n",
        "\n",
        "`num_initial_points`: \n",
        "\n",
        "Number of randomly selected hyperparameter combinations to try before applying Bayesian probability to determine the likelihood of which param combo to try next based on expected improvement\n",
        "\n",
        "\n",
        "`beta`: \n",
        "\n",
        "Larger values mean more willingness to explore new hyperparameter combinations (analogous to searching for the global minimum in gradient descent). Conversely, smaller values mean less willingness to try new hyperparameter combinations (analogous to getting stuck in a local minimum in gradient descent). \n",
        "\n",
        "As a start, err on the side of larger values. What defines a small or large value, you ask? That question would pull us into the mathematical intricacies of Bayesian Optimization and Gaussian Processes. For simplicity, notice that the default value is 2.6 and work from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_lv5tqyI0O6"
      },
      "source": [
        "# we know that 24 samples is about 25% of 96 possible hyper-parameter combos\n",
        "# because BO isn't random (after num_initial_points number of trails) let's see if 15 max trials gives good results\n",
        "# feel free to play with any of these numbers\n",
        "max_trials=15\n",
        "num_initial_points=5\n",
        "beta=5.0"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xCtk3nVI0O6"
      },
      "source": [
        "bayesian_tuner = BayesianOptimization(\n",
        "                    build_model,\n",
        "                    objective='val_accuracy',\n",
        "                    max_trials=max_trials,\n",
        "                    hyperparameters=hp, # pass in our hyperparameter dictionary\n",
        "                    num_initial_points=num_initial_points, \n",
        "                    beta=beta, \n",
        "                    seed=1234,\n",
        "                    directory='./keras-tuner-trial',\n",
        "                    project_name='bayesian_optimization_4')"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Myq4RDSgI0O6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa612611-ff0d-4373-90b3-fca7128b16b5"
      },
      "source": [
        "bayesian_tuner.search(X_train, y_train,\n",
        "               epochs=3,\n",
        "               validation_data=(X_test, y_test))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 00m 15s]\n",
            "val_accuracy: 0.8253200054168701\n",
            "\n",
            "Best val_accuracy So Far: 0.8729199767112732\n",
            "Total elapsed time: 00h 06m 56s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "4xQX_EUKI0O6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb57e77e-3cce-4d06-853a-4b8317a1b9b4"
      },
      "source": [
        "bayesian_tuner.results_summary(num_trials=1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./keras-tuner-trial/bayesian_optimization_4\n",
            "Showing 1 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 512\n",
            "learning_rate: 0.001\n",
            "activation: relu\n",
            "Score: 0.8729199767112732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R33cbZ26I0O7"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the best performing hyperparameter combination and model score. Note that because this is Bayesian Optimization, multiple runs might have slightly different outcomes.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1badcdca408cdd49bc2e409dca3bac5a",
          "grade": true,
          "grade_id": "cell-ff95600bf745f40f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Z7cbqTgNI0O7"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "* units: 512\n",
        "* learning_rate: 0.001\n",
        "* activation: relu\n",
        "\n",
        "Score: 87.29% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMwSVnUJI0O7"
      },
      "source": [
        "---------\n",
        "## Brute Force Gridsearch Optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0W4bCd_I0O7"
      },
      "source": [
        "### Populate a Sklearn Compatible Parameter Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DT_ytR-uI0O7"
      },
      "source": [
        "# build out our hyperparameter dictionary \n",
        "hyper_parameters = {\n",
        "    # BUG Fix: cast array as list otherwise GridSearchCV will throw error\n",
        "    \"units\": np.arange(32, 544, 32).tolist(),\n",
        "    \"learning_rate\": [1e-1, 1e-2, 1e-3],\n",
        "    \"activation\":[\"relu\", \"sigmoid\"]\n",
        "}"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC6we-hhI0O7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7d61fa0-cc3b-4600-c0ef-99016d5e660f"
      },
      "source": [
        "hyper_parameters"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': ['relu', 'sigmoid'],\n",
              " 'learning_rate': [0.1, 0.01, 0.001],\n",
              " 'units': [32,\n",
              "  64,\n",
              "  96,\n",
              "  128,\n",
              "  160,\n",
              "  192,\n",
              "  224,\n",
              "  256,\n",
              "  288,\n",
              "  320,\n",
              "  352,\n",
              "  384,\n",
              "  416,\n",
              "  448,\n",
              "  480,\n",
              "  512]}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c8p4RjII0O7"
      },
      "source": [
        "### Build a Sklearn Compatible Model Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irzGZqpHI0O7"
      },
      "source": [
        "def build_model(units, learning_rate, activation):\n",
        "    \n",
        "    \"\"\"\n",
        "    Returns a compile keras model ready for keras-tuner gridsearch algorithms \n",
        "    \"\"\"\n",
        "    \n",
        "    model = Sequential()\n",
        "    \n",
        "    # hidden layer\n",
        "    model.add(Dense(units, activation=activation))\n",
        "    \n",
        "    # output layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGGbBttyI0O7"
      },
      "source": [
        "model = KerasClassifier(build_fn = build_model)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "-6xsKyfLI0O7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034f2c71-6bcd-4e2b-caea-c8fab53f3737"
      },
      "source": [
        "# save start time \n",
        "start = time()\n",
        "\n",
        "# Create Grid Search\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=hyper_parameters, \n",
        "                    n_jobs=-2, \n",
        "                    verbose=1, \n",
        "                    cv=3)\n",
        "\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# save end time \n",
        "end = time()\n",
        "\n",
        "# Report Results\n",
        "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
        "\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-2)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0508 - accuracy: 0.2350\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.1498 - accuracy: 0.1882\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8662 - accuracy: 0.3211\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8976 - accuracy: 0.3098\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8537 - accuracy: 0.3263\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9185 - accuracy: 0.2672\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9670 - accuracy: 0.2793\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.0123 - accuracy: 0.2314\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8551 - accuracy: 0.3132\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9817 - accuracy: 0.2900\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.8529 - accuracy: 0.3457\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9888 - accuracy: 0.2410\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.0073 - accuracy: 0.2910\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8478 - accuracy: 0.3277\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 2.2298 - accuracy: 0.1737\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 2.4656 - accuracy: 0.1829\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.9895 - accuracy: 0.3041\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9727 - accuracy: 0.2520\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0197 - accuracy: 0.2675\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9755 - accuracy: 0.2540\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9278 - accuracy: 0.3176\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8230 - accuracy: 0.3228\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9393 - accuracy: 0.3229\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.7893 - accuracy: 0.3665\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9314 - accuracy: 0.3309\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8609 - accuracy: 0.3170\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.0152 - accuracy: 0.2973\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9263 - accuracy: 0.2919\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.8893 - accuracy: 0.3568\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.7110 - accuracy: 0.3757\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9581 - accuracy: 0.3327\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8608 - accuracy: 0.2861\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0049 - accuracy: 0.3143\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8106 - accuracy: 0.3273\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.9262 - accuracy: 0.3476\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.8234 - accuracy: 0.3135\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9323 - accuracy: 0.3621\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.4855 - accuracy: 0.2535\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 2.1258 - accuracy: 0.2784\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.9193 - accuracy: 0.2986\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9506 - accuracy: 0.3437\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9771 - accuracy: 0.2490\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9183 - accuracy: 0.3542\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8576 - accuracy: 0.3572\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.9821 - accuracy: 0.3360\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9193 - accuracy: 0.3134\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0917 - accuracy: 0.3167\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8680 - accuracy: 0.2944\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0184 - accuracy: 0.3415\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8388 - accuracy: 0.3570\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.9845 - accuracy: 0.3426\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9268 - accuracy: 0.3070\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 2.0150 - accuracy: 0.3138\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.7856 - accuracy: 0.3438\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0810 - accuracy: 0.2924\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8427 - accuracy: 0.3082\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0248 - accuracy: 0.3306\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9112 - accuracy: 0.2802\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0792 - accuracy: 0.2860\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.8085 - accuracy: 0.3146\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.1358 - accuracy: 0.2912\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.9844 - accuracy: 0.2444\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0920 - accuracy: 0.3106\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.2435 - accuracy: 0.2582\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.2289 - accuracy: 0.2386\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.2068 - accuracy: 0.1495\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0013 - accuracy: 0.4018\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8678 - accuracy: 0.2824\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 2.0132 - accuracy: 0.3426\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9791 - accuracy: 0.2260\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0966 - accuracy: 0.3226\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8196 - accuracy: 0.3278\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1323 - accuracy: 0.3765\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9421 - accuracy: 0.2739\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1074 - accuracy: 0.3263\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8075 - accuracy: 0.3234\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.1600 - accuracy: 0.3144\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0144 - accuracy: 0.2253\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.2045 - accuracy: 0.3058\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9327 - accuracy: 0.2554\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.2593 - accuracy: 0.3370\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9089 - accuracy: 0.3031\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0647 - accuracy: 0.3350\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9045 - accuracy: 0.2841\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0635 - accuracy: 0.3621\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9155 - accuracy: 0.2448\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.2506 - accuracy: 0.3706\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9772 - accuracy: 0.3316\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0991 - accuracy: 0.3767\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9535 - accuracy: 0.3455\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.1061 - accuracy: 0.3292\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.0318 - accuracy: 0.2576\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.1542 - accuracy: 0.3303\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.9588 - accuracy: 0.3042\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2100 - accuracy: 0.3161\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.8669 - accuracy: 0.3098\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7982 - accuracy: 0.7567\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7311 - accuracy: 0.7773\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7962 - accuracy: 0.7607\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7203 - accuracy: 0.7858\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7889 - accuracy: 0.7587\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6806 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7630 - accuracy: 0.7678\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6771 - accuracy: 0.7927\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7641 - accuracy: 0.7685\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7404 - accuracy: 0.7750\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7510 - accuracy: 0.7706\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6559 - accuracy: 0.8045\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7523 - accuracy: 0.7702\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6793 - accuracy: 0.7988\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7446 - accuracy: 0.7746\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6538 - accuracy: 0.8034\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7466 - accuracy: 0.7740\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6698 - accuracy: 0.8032\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7429 - accuracy: 0.7760\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6780 - accuracy: 0.7952\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7464 - accuracy: 0.7749\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6863 - accuracy: 0.7971\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7526 - accuracy: 0.7722\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6624 - accuracy: 0.8028\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7406 - accuracy: 0.7774\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6899 - accuracy: 0.7992\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7441 - accuracy: 0.7745\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6840 - accuracy: 0.7834\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7460 - accuracy: 0.7734\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6619 - accuracy: 0.8074\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7405 - accuracy: 0.7745\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6718 - accuracy: 0.8067\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7400 - accuracy: 0.7764\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6687 - accuracy: 0.7963\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7435 - accuracy: 0.7750\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6559 - accuracy: 0.8052\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7397 - accuracy: 0.7767\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6617 - accuracy: 0.8078\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7448 - accuracy: 0.7750\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6498 - accuracy: 0.8085\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7419 - accuracy: 0.7773\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6722 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7439 - accuracy: 0.7762\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6442 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7339 - accuracy: 0.7783\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6635 - accuracy: 0.8019\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7491 - accuracy: 0.7742\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6863 - accuracy: 0.8047\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7433 - accuracy: 0.7771\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7019 - accuracy: 0.7890\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7451 - accuracy: 0.7768\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6510 - accuracy: 0.8098\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7461 - accuracy: 0.7755\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6810 - accuracy: 0.7951\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7496 - accuracy: 0.7724\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.8001\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7458 - accuracy: 0.7770\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6834 - accuracy: 0.7908\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7370 - accuracy: 0.7806\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6509 - accuracy: 0.8110\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7415 - accuracy: 0.7779\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6854 - accuracy: 0.8051\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.7411 - accuracy: 0.7755\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6894 - accuracy: 0.7953\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7399 - accuracy: 0.7774\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6623 - accuracy: 0.8016\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7435 - accuracy: 0.7762\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6719 - accuracy: 0.8013\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7484 - accuracy: 0.7739\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6591 - accuracy: 0.8046\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7444 - accuracy: 0.7764\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6506 - accuracy: 0.8116\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7419 - accuracy: 0.7765\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6557 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7366 - accuracy: 0.7787\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6694 - accuracy: 0.8053\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7391 - accuracy: 0.7792\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6548 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7370 - accuracy: 0.7780\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6618 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7410 - accuracy: 0.7763\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6446 - accuracy: 0.8080\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7539 - accuracy: 0.7733\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6684 - accuracy: 0.8056\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7433 - accuracy: 0.7772\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6788 - accuracy: 0.7983\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7425 - accuracy: 0.7784\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6630 - accuracy: 0.7998\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.7446 - accuracy: 0.7780\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6305 - accuracy: 0.8160\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7464 - accuracy: 0.7766\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6818 - accuracy: 0.7913\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7445 - accuracy: 0.7791\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6780 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7509 - accuracy: 0.7754\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6357 - accuracy: 0.8158\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8726 - accuracy: 0.7418\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7200 - accuracy: 0.7918\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8704 - accuracy: 0.7445\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7178 - accuracy: 0.7885\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8712 - accuracy: 0.7419\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7082 - accuracy: 0.7918\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7950 - accuracy: 0.7654\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6516 - accuracy: 0.8105\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8076 - accuracy: 0.7591\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6428 - accuracy: 0.8082\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7941 - accuracy: 0.7628\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6466 - accuracy: 0.8136\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7655 - accuracy: 0.7724\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6252 - accuracy: 0.8159\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7720 - accuracy: 0.7708\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6132 - accuracy: 0.8160\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7560 - accuracy: 0.7731\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6237 - accuracy: 0.8161\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7407 - accuracy: 0.7790\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5959 - accuracy: 0.8244\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7487 - accuracy: 0.7798\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6057 - accuracy: 0.8206\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7380 - accuracy: 0.7795\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5928 - accuracy: 0.8242\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7270 - accuracy: 0.7832\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5829 - accuracy: 0.8298\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7322 - accuracy: 0.7822\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.5834 - accuracy: 0.8253\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7240 - accuracy: 0.7843\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5692 - accuracy: 0.8356\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7087 - accuracy: 0.7890\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5668 - accuracy: 0.8356\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7205 - accuracy: 0.7845\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5772 - accuracy: 0.8266\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7130 - accuracy: 0.7871\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5634 - accuracy: 0.8369\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6989 - accuracy: 0.7918\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5689 - accuracy: 0.8303\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.7023 - accuracy: 0.7913\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5468 - accuracy: 0.8380\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.7104 - accuracy: 0.7872\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5631 - accuracy: 0.8329\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6946 - accuracy: 0.7928\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5565 - accuracy: 0.8351\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6998 - accuracy: 0.7929\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5611 - accuracy: 0.8305\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6966 - accuracy: 0.7923\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5545 - accuracy: 0.8343\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6871 - accuracy: 0.7950\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5414 - accuracy: 0.8410\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6896 - accuracy: 0.7966\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.8410\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6881 - accuracy: 0.7939\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5474 - accuracy: 0.8400\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6801 - accuracy: 0.7984\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5490 - accuracy: 0.8385\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6846 - accuracy: 0.7947\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5495 - accuracy: 0.8356\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6849 - accuracy: 0.7946\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5483 - accuracy: 0.8377\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6741 - accuracy: 0.7995\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.8425\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6804 - accuracy: 0.7983\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5417 - accuracy: 0.8376\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6801 - accuracy: 0.7962\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5345 - accuracy: 0.8441\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6698 - accuracy: 0.8000\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5439 - accuracy: 0.8378\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6766 - accuracy: 0.7985\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5671 - accuracy: 0.8230\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6705 - accuracy: 0.8000\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5451 - accuracy: 0.8377\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6674 - accuracy: 0.8011\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5401 - accuracy: 0.8391\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6718 - accuracy: 0.8007\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5245 - accuracy: 0.8438\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6734 - accuracy: 0.7968\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5416 - accuracy: 0.8405\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6655 - accuracy: 0.8014\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5409 - accuracy: 0.8386\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6663 - accuracy: 0.8022\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5262 - accuracy: 0.8428\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6680 - accuracy: 0.7998\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5194 - accuracy: 0.8482\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6612 - accuracy: 0.8031\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5246 - accuracy: 0.8425\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6657 - accuracy: 0.8024\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5285 - accuracy: 0.8458\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6671 - accuracy: 0.8000\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5380 - accuracy: 0.8422\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6544 - accuracy: 0.8048\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5308 - accuracy: 0.8419\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6590 - accuracy: 0.8040\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5176 - accuracy: 0.8428\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6662 - accuracy: 0.8008\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5376 - accuracy: 0.8404\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1665 - accuracy: 0.6230\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0831 - accuracy: 0.6592\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1986 - accuracy: 0.6201\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1816 - accuracy: 0.6266\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1239 - accuracy: 0.6376\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0725 - accuracy: 0.6818\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1796 - accuracy: 0.6294\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1518 - accuracy: 0.6443\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1836 - accuracy: 0.6331\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1906 - accuracy: 0.6198\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.1098 - accuracy: 0.6464\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0055 - accuracy: 0.6805\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1904 - accuracy: 0.6246\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0733 - accuracy: 0.6706\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2208 - accuracy: 0.6238\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0733 - accuracy: 0.6933\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1768 - accuracy: 0.6268\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0609 - accuracy: 0.6870\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1661 - accuracy: 0.6329\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0577 - accuracy: 0.6540\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.3121 - accuracy: 0.6079\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.1496 - accuracy: 0.6554\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.1621 - accuracy: 0.6323\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 1.0545 - accuracy: 0.6865\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2638 - accuracy: 0.6229\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0287 - accuracy: 0.6933\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 1.2810 - accuracy: 0.6153\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1363 - accuracy: 0.6544\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2854 - accuracy: 0.6132\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2299 - accuracy: 0.6208\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3613 - accuracy: 0.6020\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.4636 - accuracy: 0.5698\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3477 - accuracy: 0.6146\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3319 - accuracy: 0.6292\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2249 - accuracy: 0.6226\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0194 - accuracy: 0.6828\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.1622 - accuracy: 0.6401\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0287 - accuracy: 0.6944\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2997 - accuracy: 0.6255\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2484 - accuracy: 0.6243\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3455 - accuracy: 0.5983\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1764 - accuracy: 0.6560\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2969 - accuracy: 0.6235\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0600 - accuracy: 0.6833\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5223 - accuracy: 0.5886\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.1682 - accuracy: 0.6498\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3183 - accuracy: 0.6213\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3409 - accuracy: 0.6567\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4083 - accuracy: 0.6077\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.0846 - accuracy: 0.6765\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2511 - accuracy: 0.6327\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3495 - accuracy: 0.6062\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2511 - accuracy: 0.6276\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.2323 - accuracy: 0.6431\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4869 - accuracy: 0.5974\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 2.1541 - accuracy: 0.5347\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5601 - accuracy: 0.5857\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 1.3460 - accuracy: 0.6520\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6749 - accuracy: 0.5791\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7113 - accuracy: 0.5965\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.2327 - accuracy: 0.6307\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0830 - accuracy: 0.6850\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.6440 - accuracy: 0.5876\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2532 - accuracy: 0.6692\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 1.5508 - accuracy: 0.5892\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.4403 - accuracy: 0.6082\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.3806 - accuracy: 0.6159\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2576 - accuracy: 0.6669\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 1.1693 - accuracy: 0.6469\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.0568 - accuracy: 0.6778\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 2.0558 - accuracy: 0.5348\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6125 - accuracy: 0.6016\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5431 - accuracy: 0.6042\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2971 - accuracy: 0.6450\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6283 - accuracy: 0.5747\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5651 - accuracy: 0.5937\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4127 - accuracy: 0.6139\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6050 - accuracy: 0.6089\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.7227 - accuracy: 0.5792\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.1680 - accuracy: 0.6671\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.5434 - accuracy: 0.5947\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.6079 - accuracy: 0.5969\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6728 - accuracy: 0.6023\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.7583 - accuracy: 0.6309\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.4077 - accuracy: 0.6068\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5616 - accuracy: 0.5826\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.6046 - accuracy: 0.5997\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5098 - accuracy: 0.6102\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7507 - accuracy: 0.5861\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.3186 - accuracy: 0.5380\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8177 - accuracy: 0.5689\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 2.1539 - accuracy: 0.5418\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6006 - accuracy: 0.5925\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.5724 - accuracy: 0.6058\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6293 - accuracy: 0.5979\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 1.2622 - accuracy: 0.6682\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7958 - accuracy: 0.7584\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6803 - accuracy: 0.7974\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7905 - accuracy: 0.7627\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7077 - accuracy: 0.7849\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7926 - accuracy: 0.7605\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6856 - accuracy: 0.7967\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7282 - accuracy: 0.7763\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6346 - accuracy: 0.8060\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7338 - accuracy: 0.7766\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6390 - accuracy: 0.8051\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7338 - accuracy: 0.7769\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6393 - accuracy: 0.8068\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7087 - accuracy: 0.7822\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6221 - accuracy: 0.8135\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7022 - accuracy: 0.7837\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6082 - accuracy: 0.8175\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7142 - accuracy: 0.7809\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6019 - accuracy: 0.8189\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.6936 - accuracy: 0.7866\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6006 - accuracy: 0.8201\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7053 - accuracy: 0.7838\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6229 - accuracy: 0.8142\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.7064 - accuracy: 0.7809\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6013 - accuracy: 0.8202\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6933 - accuracy: 0.7859\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6117 - accuracy: 0.8169\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6872 - accuracy: 0.7883\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.8139\n",
            "1563/1563 [==============================] - 4s 3ms/step - loss: 0.6931 - accuracy: 0.7847\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6005 - accuracy: 0.8203\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6868 - accuracy: 0.7892\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6475 - accuracy: 0.8008\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6997 - accuracy: 0.7846\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6157 - accuracy: 0.8065\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6964 - accuracy: 0.7848\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5898 - accuracy: 0.8221\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6837 - accuracy: 0.7895\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5964 - accuracy: 0.8192\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6882 - accuracy: 0.7881\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5815 - accuracy: 0.8203\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6941 - accuracy: 0.7856\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6357 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6835 - accuracy: 0.7880\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5843 - accuracy: 0.8228\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6860 - accuracy: 0.7887\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6048 - accuracy: 0.8136\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6823 - accuracy: 0.7890\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6071 - accuracy: 0.8172\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6883 - accuracy: 0.7890\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5799 - accuracy: 0.8204\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6870 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6117 - accuracy: 0.8126\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6847 - accuracy: 0.7886\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.5795 - accuracy: 0.8240\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6901 - accuracy: 0.7865\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5811 - accuracy: 0.8240\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6813 - accuracy: 0.7907\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5805 - accuracy: 0.8246\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6865 - accuracy: 0.7882\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5688 - accuracy: 0.8303\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.6880 - accuracy: 0.7881\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6111 - accuracy: 0.8177\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6811 - accuracy: 0.7902\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6222 - accuracy: 0.8118\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.6894 - accuracy: 0.7883\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5772 - accuracy: 0.8248\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6826 - accuracy: 0.7903\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6030 - accuracy: 0.8145\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.6887 - accuracy: 0.7879\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6222 - accuracy: 0.8168\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6923 - accuracy: 0.7858\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6080 - accuracy: 0.8191\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6890 - accuracy: 0.7877\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5773 - accuracy: 0.8251\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6936 - accuracy: 0.7877\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6020 - accuracy: 0.8155\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6916 - accuracy: 0.7859\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6304 - accuracy: 0.8024\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6970 - accuracy: 0.7850\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5975 - accuracy: 0.8169\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6917 - accuracy: 0.7866\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6031 - accuracy: 0.8184\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6944 - accuracy: 0.7851\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5930 - accuracy: 0.8238\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6909 - accuracy: 0.7877\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6030 - accuracy: 0.8157\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6870 - accuracy: 0.7900\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6223 - accuracy: 0.8110\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6863 - accuracy: 0.7896\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6126 - accuracy: 0.8201\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6859 - accuracy: 0.7883\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6476 - accuracy: 0.8064\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6913 - accuracy: 0.7871\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6005 - accuracy: 0.8223\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6901 - accuracy: 0.7884\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.5936 - accuracy: 0.8210\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0503 - accuracy: 0.6970\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8234 - accuracy: 0.7574\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0377 - accuracy: 0.7101\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8076 - accuracy: 0.7602\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 1.0419 - accuracy: 0.7029\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.8116 - accuracy: 0.7662\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9291 - accuracy: 0.7302\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7614 - accuracy: 0.7724\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9389 - accuracy: 0.7281\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7455 - accuracy: 0.7770\n",
            "1563/1563 [==============================] - 3s 2ms/step - loss: 0.9466 - accuracy: 0.7235\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7517 - accuracy: 0.7823\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8925 - accuracy: 0.7379\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7289 - accuracy: 0.7828\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8982 - accuracy: 0.7396\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7304 - accuracy: 0.7836\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8932 - accuracy: 0.7403\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7200 - accuracy: 0.7904\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8689 - accuracy: 0.7438\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7107 - accuracy: 0.7939\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8746 - accuracy: 0.7445\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.7049 - accuracy: 0.7930\n",
            "1563/1563 [==============================] - 4s 2ms/step - loss: 0.8686 - accuracy: 0.7430\n",
            "782/782 [==============================] - 1s 1ms/step - loss: 0.6966 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8534 - accuracy: 0.7485\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.7005 - accuracy: 0.7919\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8556 - accuracy: 0.7491\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6949 - accuracy: 0.7946\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8629 - accuracy: 0.7428\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6970 - accuracy: 0.7976\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8468 - accuracy: 0.7504\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6991 - accuracy: 0.7944\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8474 - accuracy: 0.7527\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6925 - accuracy: 0.7921\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8459 - accuracy: 0.7507\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6820 - accuracy: 0.8020\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8427 - accuracy: 0.7487\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6929 - accuracy: 0.7983\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8444 - accuracy: 0.7514\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6922 - accuracy: 0.7960\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8415 - accuracy: 0.7505\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6936 - accuracy: 0.7960\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8296 - accuracy: 0.7552\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6963 - accuracy: 0.7959\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8334 - accuracy: 0.7538\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6834 - accuracy: 0.7992\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8414 - accuracy: 0.7500\n",
            "782/782 [==============================] - 1s 2ms/step - loss: 0.6901 - accuracy: 0.7995\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8264 - accuracy: 0.7556\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6801 - accuracy: 0.7989\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8318 - accuracy: 0.7550\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6892 - accuracy: 0.7950\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8303 - accuracy: 0.7517\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6808 - accuracy: 0.8036\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8231 - accuracy: 0.7567\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6762 - accuracy: 0.7992\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8272 - accuracy: 0.7539\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6683 - accuracy: 0.8010\n",
            "1563/1563 [==============================] - 5s 3ms/step - loss: 0.8261 - accuracy: 0.7538\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6715 - accuracy: 0.8035\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8226 - accuracy: 0.7557\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6734 - accuracy: 0.8019\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8283 - accuracy: 0.7534\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6783 - accuracy: 0.7994\n",
            "1563/1563 [==============================] - 6s 3ms/step - loss: 0.8273 - accuracy: 0.7515\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6696 - accuracy: 0.8058\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8218 - accuracy: 0.7554\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6685 - accuracy: 0.8044\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8249 - accuracy: 0.7555\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6836 - accuracy: 0.7941\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8259 - accuracy: 0.7535\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6665 - accuracy: 0.8067\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8174 - accuracy: 0.7564\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6753 - accuracy: 0.8003\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8191 - accuracy: 0.7573\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6765 - accuracy: 0.7997\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 0.8219 - accuracy: 0.7533\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6732 - accuracy: 0.8040\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8158 - accuracy: 0.7582\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6767 - accuracy: 0.7963\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8173 - accuracy: 0.7580\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6663 - accuracy: 0.8014\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8176 - accuracy: 0.7555\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6751 - accuracy: 0.7982\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8177 - accuracy: 0.7548\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6770 - accuracy: 0.7962\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8217 - accuracy: 0.7548\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6762 - accuracy: 0.7997\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8183 - accuracy: 0.7546\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6638 - accuracy: 0.8052\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8152 - accuracy: 0.7574\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6686 - accuracy: 0.8048\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8130 - accuracy: 0.7573\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.6698 - accuracy: 0.7999\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8183 - accuracy: 0.7528\n",
            "782/782 [==============================] - 2s 2ms/step - loss: 0.6556 - accuracy: 0.8096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-2)]: Done 288 out of 288 | elapsed: 37.4min finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2344/2344 [==============================] - 9s 4ms/step - loss: 0.6145 - accuracy: 0.8166\n",
            "Best: 0.843506654103597 using {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.2550533364216487, Stdev: 0.0503435259639316 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.25413333376248676, Stdev: 0.025667599231628053 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.2541999965906143, Stdev: 0.05913407729191842 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.31441332896550495, Stdev: 0.046329882073370555 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.3281866709391276, Stdev: 0.03513606091565234 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.30896000067392987, Stdev: 0.01713684653802288 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.2670133262872696, Stdev: 0.022381714794439872 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.3216799994309743, Stdev: 0.026291734388158513 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.3359333276748657, Stdev: 0.02117997800213421 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.3009999990463257, Stdev: 0.01496309335581122 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.2173600047826767, Stdev: 0.0483317354193148 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.27873333791891736, Stdev: 0.04162595106387799 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.2741999973853429, Stdev: 0.040041629319969005 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.28089332580566406, Stdev: 0.019598535868654007 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.3072800040245056, Stdev: 0.04456999024386337 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.29055999716122943, Stdev: 0.023418092833773003 with: {'activation': 'relu', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.7882000009218851, Stdev: 0.00998734428401891 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.790720005830129, Stdev: 0.012147155847214562 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8017866611480713, Stdev: 0.0021407353008930994 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.7983733216921488, Stdev: 0.003219006522170066 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.7966933250427246, Stdev: 0.009956988393523538 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8027600049972534, Stdev: 0.004593667794727803 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8070266644159952, Stdev: 0.001654526933611099 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.802839994430542, Stdev: 0.001301482958733984 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.7979466517766317, Stdev: 0.008723623448842565 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8006399869918823, Stdev: 0.008272431214815357 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8006666501363119, Stdev: 0.004071018274947015 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8058000008265177, Stdev: 0.004287388629248022 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8059200048446655, Stdev: 0.00044898737102044107 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8065333366394043, Stdev: 0.0010498854884007228 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8047199845314026, Stdev: 0.008027216267809846 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.8035733501116434, Stdev: 0.009993929907528062 with: {'activation': 'relu', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.7907066742579142, Stdev: 0.0015462273823696657 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.8107333381970724, Stdev: 0.002211807876717521 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.8159733215967814, Stdev: 8.218133044227328e-05 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.8230666518211365, Stdev: 0.0017461158567058544 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.83024001121521, Stdev: 0.004208314645700284 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.8330666820208231, Stdev: 0.004574289915123852 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.8337200085322062, Stdev: 0.0032071545680165763 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.8332933386166891, Stdev: 0.00198807577417246 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.8406800230344137, Stdev: 0.0004537214409600001 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8372666637102762, Stdev: 0.0012501920635791285 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8414133389790853, Stdev: 0.002774434327083983 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8328266739845276, Stdev: 0.006948670855906028 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8411333362261454, Stdev: 0.0019656712041584653 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.8431866765022278, Stdev: 0.0039450145740140335 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.843506654103597, Stdev: 0.001656441949763796 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.8417066733042399, Stdev: 0.0009712507011148972 with: {'activation': 'relu', 'learning_rate': 0.001, 'units': 512}\n",
            "Means: 0.6558533509572347, Stdev: 0.0226562721756221 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 32}\n",
            "Means: 0.6482266783714294, Stdev: 0.024926041166369213 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 64}\n",
            "Means: 0.683626651763916, Stdev: 0.009538539461003534 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 96}\n",
            "Means: 0.6652799844741821, Stdev: 0.015002187201124652 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 128}\n",
            "Means: 0.6561733285586039, Stdev: 0.029617589373500047 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 160}\n",
            "Means: 0.6272666652997335, Stdev: 0.04613682399433232 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 192}\n",
            "Means: 0.6582533319791158, Stdev: 0.028652831784246154 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 224}\n",
            "Means: 0.663266658782959, Stdev: 0.014455344914248347 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 256}\n",
            "Means: 0.6419333219528198, Stdev: 0.028703963498988384 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 288}\n",
            "Means: 0.5944133400917053, Stdev: 0.047935169336784246 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 320}\n",
            "Means: 0.6541066765785217, Stdev: 0.033095581485099286 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 352}\n",
            "Means: 0.6487600008646647, Stdev: 0.033643834605143336 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 384}\n",
            "Means: 0.6158799926439921, Stdev: 0.0215374480974304 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 416}\n",
            "Means: 0.6316133340199789, Stdev: 0.028663707108204928 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 448}\n",
            "Means: 0.5769599874814352, Stdev: 0.029764084173350247 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 480}\n",
            "Means: 0.6052800019582113, Stdev: 0.05160369752398419 with: {'activation': 'sigmoid', 'learning_rate': 0.1, 'units': 512}\n",
            "Means: 0.793013314406077, Stdev: 0.005731258596305253 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 32}\n",
            "Means: 0.8059999942779541, Stdev: 0.0007027609098097349 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 64}\n",
            "Means: 0.8166133562723795, Stdev: 0.0022881353249959826 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 96}\n",
            "Means: 0.8181466460227966, Stdev: 0.0027909024804480343 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 128}\n",
            "Means: 0.8170533378918966, Stdev: 0.0026144856688253607 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 160}\n",
            "Means: 0.8098000089327494, Stdev: 0.008991789496795764 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 192}\n",
            "Means: 0.8152933319409689, Stdev: 0.006333342830585431 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 224}\n",
            "Means: 0.8178799947102865, Stdev: 0.003750621111762321 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 256}\n",
            "Means: 0.8190000057220459, Stdev: 0.0047481587011719605 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 288}\n",
            "Means: 0.8263066609700521, Stdev: 0.0028514044795452296 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 320}\n",
            "Means: 0.8180800080299377, Stdev: 0.005331068950388576 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 352}\n",
            "Means: 0.8167866667111715, Stdev: 0.0018943590232746697 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 384}\n",
            "Means: 0.8143466512362162, Stdev: 0.00929368151230496 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 416}\n",
            "Means: 0.8196799755096436, Stdev: 0.002944279962158672 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 448}\n",
            "Means: 0.8155866662661234, Stdev: 0.003707479319081795 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 480}\n",
            "Means: 0.8165466586748759, Stdev: 0.007194979670003147 with: {'activation': 'sigmoid', 'learning_rate': 0.01, 'units': 512}\n",
            "Means: 0.7612533370653788, Stdev: 0.003642917128100503 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 32}\n",
            "Means: 0.7772400180498759, Stdev: 0.004035974494787172 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 64}\n",
            "Means: 0.7855866750081381, Stdev: 0.003394297973168557 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 96}\n",
            "Means: 0.7954266667366028, Stdev: 0.0028626200209428882 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 128}\n",
            "Means: 0.7946799993515015, Stdev: 0.002336704515932765 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 160}\n",
            "Means: 0.7961466511090597, Stdev: 0.004218355503176537 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 192}\n",
            "Means: 0.7967466711997986, Stdev: 0.001112508213293559 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 224}\n",
            "Means: 0.7982133428255717, Stdev: 0.0016538643428164227 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 256}\n",
            "Means: 0.7991600036621094, Stdev: 0.0035491171429509706 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 288}\n",
            "Means: 0.8012266755104065, Stdev: 0.0017685761722145794 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 320}\n",
            "Means: 0.8023733297983805, Stdev: 0.0026323992975798606 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 352}\n",
            "Means: 0.8017066717147827, Stdev: 0.0054754015463620844 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 384}\n",
            "Means: 0.8013333280881246, Stdev: 0.001929526469615058 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 416}\n",
            "Means: 0.7986266811688741, Stdev: 0.002099983009158367 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 448}\n",
            "Means: 0.8003999789555868, Stdev: 0.0037055637870084274 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 480}\n",
            "Means: 0.804746667544047, Stdev: 0.003935515962081299 with: {'activation': 'sigmoid', 'learning_rate': 0.001, 'units': 512}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9xIXvLHI0O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a17faaf-89d6-480a-8bfa-d4b93686aac1"
      },
      "source": [
        "# total run time \n",
        "total_run_time_in_minutes = (end - start)/60\n",
        "total_run_time_in_minutes"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37.58189069032669"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDhmB1KCI0O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17879ea7-aef1-4ea5-f351-cfbb75a016a6"
      },
      "source": [
        "grid_result.best_params_"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation': 'relu', 'learning_rate': 0.001, 'units': 480}"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfKXnHB5I0O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "697a5bbb-e132-400d-befa-8d01b009534d"
      },
      "source": [
        "# because all other optimization approaches are reporting test set score\n",
        "# let's calculate the test set score in this case \n",
        "best_model = grid_result.best_estimator_\n",
        "test_acc = best_model.score(X_test, y_test)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 2s 2ms/step - loss: 0.4877 - accuracy: 0.8563\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFg1V-6JI0O8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ae4d1af-fda2-4405-aa83-ae55875143b9"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8562800288200378"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6aG0L0hI0O8"
      },
      "source": [
        " ### Results\n",
        " \n",
        "Identify and write the best performing hyperparameter combination and model score.\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9577db883482c6cded3836e5cfbf5a74",
          "grade": true,
          "grade_id": "cell-eb06d682d2790f6e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Kd07_qtdI0O8"
      },
      "source": [
        "YOUR ANSWER HERE\n",
        "\n",
        "* units: 480\n",
        "* learning_rate: 0.001\n",
        "* activation: relu\n",
        "\n",
        "Score: 85.63% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkIWTM_RI0O8"
      },
      "source": [
        "_______\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "The spirit of this experiment is to expose you to the idea of benchmarking and comparing the trade-offs of various gridsearch approaches. \n",
        "\n",
        "Even if we found a way to pass the original test set into GridSearchCV, we could see that both Random Search and Bayesian Optimization are arguably better alternatives to a brute force grid search. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5hq4bEgI0O8"
      },
      "source": [
        "----\n",
        "\n",
        "# Stretch Goals\n",
        "\n",
        "- Feel free to run whatever gridsearch experiments on whatever models you like!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC9jYKevI0O8"
      },
      "source": [
        "# this is your open playground - be free to explore as you wish "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}