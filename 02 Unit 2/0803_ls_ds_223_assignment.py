# -*- coding: utf-8 -*-
"""0802 LS_DS_223_assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nyqCdEEUC_U4C2SSmt14LkWV661koOMN

Lambda School Data Science

*Unit 2, Sprint 2, Module 3*

---
<p style="padding: 10px; border: 2px solid red;">
    <b>Before you start:</b> Today is the day you should submit the dataset for your Unit 2 Build Week project. You can review the guidelines and make your submission in the Build Week course for your cohort on Canvas.</p>
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import sys
# 
# # If you're on Colab:
# if 'google.colab' in sys.modules:
#     DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/main/data/'
#     !pip install category_encoders==2.*
#     !pip install pandas-profiling==2.*
# 
# # If you're working locally:
# else:
#     DATA_PATH = '../data/'

def color_percent(floaty):
  "Input a normalized number between 0 and 1, and get it in string form colored according to percentage."
  if floaty >= .5:
    green = 200
    red = int(500 * (1 - floaty))
  else:
    red = 255
    green = int(400 * floaty)

  return f'\033[38;2;{red};{green};0m{floaty}\033[00m'

"""# Module Project: Hyperparameter Tuning

This sprint, the module projects will focus on creating and improving a model for the Tanazania Water Pump dataset. Your goal is to create a model to predict whether a water pump is functional, non-functional, or needs repair.

Dataset source: [DrivenData.org](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/).

## Directions

The tasks for this project are as follows:

- **Task 1:** Use `wrangle` function to import training and test data.
- **Task 2:** Split training data into feature matrix `X` and target vector `y`.
- **Task 3:** Establish the baseline accuracy score for your dataset.
- **Task 4:** Build `clf_dt`.
- **Task 5:** Build `clf_rf`.
- **Task 6:** Evaluate classifiers using k-fold cross-validation.
- **Task 7:** Tune hyperparameters for best performing classifier.
- **Task 8:** Print out best score and params for model.
- **Task 9:** Create `submission.csv` and upload to Kaggle.

You should limit yourself to the following libraries for this project:

- `category_encoders`
- `matplotlib`
- `pandas`
- `pandas-profiling`
- `sklearn`

# I. Wrangle Data
"""

import pandas as pd

def wrangle(fm_path, tv_path=None):
  if tv_path:
    df = pd.merge(pd.read_csv(fm_path, 
                              na_values=[0, -2.000000e-08],
                              parse_dates=['date_recorded']),
                  pd.read_csv(tv_path)).set_index('id')
  else:
    df = pd.read_csv(fm_path, 
                     na_values=[0, -2.000000e-08],
                     parse_dates=['date_recorded'],
                     index_col='id')

  # Drop constant columns
  df.drop(columns=['recorded_by'], inplace=True)

  # Create age feature
  df['pump_age'] = df['date_recorded'].dt.year - df['construction_year']
  df.drop(columns='date_recorded', inplace=True)

  # Drop HCCCs
  cutoff = 100
  drop_cols = [col for col in df.select_dtypes('object').columns
              if df[col].nunique() > cutoff]
  df.drop(columns=drop_cols, inplace=True)

  # Drop duplicate columns
  dupe_cols = [col for col in df.head(15).T.duplicated().index
               if df.head(15).T.duplicated()[col]]
  df.drop(columns=dupe_cols, inplace=True)    

  # Change region and district to string
  df['region_code'] = df['region_code'].astype(str)         
  df['district_code'] = df['district_code'].astype(str)

  remove_cols = [
    #   'num_private',
    'construction_year',
    'basin'
  ]

  df = df.drop(columns=remove_cols)

  return df

"""**Task 1:** Using the above `wrangle` function to read `train_features.csv` and `train_labels.csv` into the DataFrame `df`, and `test_features.csv` into the DataFrame `X_test`."""

df = wrangle('02 Unit 2/files/train_features.csv', '02 Unit 2/files/train_labels.csv')
X_test = wrangle('02 Unit 2/files/test_features.csv')

print(df.info())

"""# II. Split Data

**Task 2:** Split your DataFrame `df` into a feature matrix `X` and the target vector `y`. You want to predict `'status_group'`.

**Note:** You won't need to do a train-test split because you'll use cross-validation instead.
"""

target = 'status_group'
X = df.drop(columns=target)
y = df[target]

"""# III. Establish Baseline

**Task 3:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents.
"""

baseline_acc = y.value_counts(normalize=True).max()
print('Baseline Accuracy Score:', color_percent(baseline_acc))

"""# IV. Build Models

**Task 4:** Build a `Pipeline` named `clf_dt`. Your `Pipeline` should include:

- an `OrdinalEncoder` transformer for categorical features.
- a `SimpleImputer` transformer fot missing values.
- a `DecisionTreeClassifier` Predictor.

**Note:** Do not train `clf_dt`. You'll do that in a subsequent task. 
"""

from category_encoders import OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import make_pipeline

clf_dt = make_pipeline(
    OrdinalEncoder(),
    SimpleImputer(strategy='mean'),
    DecisionTreeClassifier(
        random_state=42
    )
)

"""**Task 5:** Build a `Pipeline` named `clf_rf`. Your `Pipeline` should include:

- an `OrdinalEncoder` transformer for categorical features.
- a `SimpleImputer` transformer fot missing values.
- a `RandomForestClassifier` predictor.

**Note:** Do not train `clf_rf`. You'll do that in a subsequent task. 
"""

from sklearn.ensemble import RandomForestClassifier

clf_rf = make_pipeline(
    OrdinalEncoder(),
    SimpleImputer(strategy='median'),
    RandomForestClassifier(
        random_state=42,
        n_jobs=-1,
        # criterion='entropy',
        # max_depth=25,
        # n_estimators=164,
        min_samples_leaf=2
    )
)

"""# V. Check Metrics

**Task 6:** Evaluate the performance of both of your classifiers using k-fold cross-validation.
"""

from sklearn.model_selection import cross_val_score

cv_scores_dt = cross_val_score(clf_dt, X, y, cv=5)
cv_scores_rf = cross_val_score(clf_rf, X, y, cv=5, n_jobs=-1)

print('CV scores DecisionTreeClassifier')
print(cv_scores_dt)
print('Mean CV accuracy score:', color_percent(cv_scores_dt.mean()))
print('STD CV accuracy score:', cv_scores_dt.std())

print('CV score RandomForestClassifier')
print(cv_scores_rf)
print('Mean CV accuracy score:', color_percent(cv_scores_rf.mean()))
print('STD CV accuracy score:', cv_scores_rf.std())

"""# VI. Tune Model

**Task 7:** Choose the best performing of your two models and tune its hyperparameters using a `RandomizedSearchCV` named `model`. Make sure that you include cross-validation and that `n_iter` is set to at least `25`.

**Note:** If you're not sure which hyperparameters to tune, check the notes from today's guided project and the `sklearn` documentation. 
"""

# exit()

from sklearn.model_selection import GridSearchCV

model = make_pipeline(
    OrdinalEncoder(),
    SimpleImputer(),
    RandomForestClassifier(random_state=42, n_jobs=-1)
)

import numpy as np

param_grid = {
    'simpleimputer__strategy': ['mean', 'median'],
    'randomforestclassifier__max_depth': range(5,35,5),
    'randomforestclassifier__n_estimators': range(20, 180, 20),
    'randomforestclassifier__min_samples_leaf': range(1,3,1),
    'randomforestclassifier__criterion': ['gini','entropy']#,
    # 'randomforestclassifier__max_samples': np.arange(.1, 1.0, .1),
    # 'randomforestclassifier__max_features': range(2, 27, 5)
}

# param_grid = {'randomforestclassifier__criterion': ['gini'], 'randomforestclassifier__max_depth': [25], 'randomforestclassifier__min_samples_leaf': [2], 'randomforestclassifier__n_estimators': [140], 'simpleimputer__strategy': ['mean']}

# param_grid = {
#     'simpleimputer__strategy': ['median'],
#     'randomforestclassifier__max_depth': [25],
#     'randomforestclassifier__n_estimators': [164],
#     'randomforestclassifier__min_samples_leaf': [2],
#     'randomforestclassifier__criterion': ['gini']
# }

# param_grid = {
#     'simpleimputer__strategy': ['mean'],
#     'randomforestclassifier__max_depth': [26],
#     'randomforestclassifier__n_estimators': range(136, 145, 1),
#     'randomforestclassifier__min_samples_leaf': [2],
#     'randomforestclassifier__criterion': ['gini']
# }

model_rfgs = GridSearchCV(
    model,
    param_grid=param_grid,
    n_jobs=-1,
    cv=5,
    verbose=1
)

model_rfgs.fit(X,y)

"""**Task 8:** Print out the best score and best params for `model`."""

best_score = model_rfgs.best_score_
best_params = model_rfgs.best_params_

print('Best score for `model`:', color_percent(best_score))
print('Best params for `model`:', best_params)

# Best score for `model`: 0.8073191223328022
# Best params for `model`: {'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 25, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 150, 'simpleimputer__strategy': 'median'}

# Best score for `model`: 0.8074874636434144
# Best params for `model`: {'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 25, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 160, 'simpleimputer__strategy': 'median'}

# Best score for `model`: 0.8076137373418495
# Best params for `model`: {'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 25, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 164, 'simpleimputer__strategy': 'median'}

### With pump age
# Best score for `model`: 0.8061407043708686
# Best params for `model`: {'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 25, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 130, 'simpleimputer__strategy': 'median'}

# Best score for `model`: 0.8063511531534788
# Best params for `model`: {'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 25, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 126, 'simpleimputer__strategy': 'median'}

# Best score for `model`: 0.8063931831202694
# Best params for `model`: {'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 27, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 125, 'simpleimputer__strategy': 'median'}

### With district and region
# Best score for `model`: 0.8058249835866116
# Best params for `model`: {'randomforestclassifier__criterion': 'gini', 'randomforestclassifier__max_depth': 25, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 140, 'simpleimputer__strategy': 'mean'}

# Best score for `model`: 0.8060143863837436
# Best params for `model`: {'randomforestclassifier__criterion': 'gini', 'randomforestclassifier__max_depth': 26, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 140, 'simpleimputer__strategy': 'mean'}

### With new parameters and columns dropped
# Best score for `model`: 0.8073402170358396
# Best params for `model`: {'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__max_depth': 26, 'randomforestclassifier__max_features': 7, 'randomforestclassifier__max_samples': 0.8999999999999999, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 145, 'simpleimputer__strategy': 'median'}

# Best score for `model`: 0.8074244009777527
# Best params for `model`: {'randomforestclassifier__criterion': 'gini', 'randomforestclassifier__max_depth': 25, 'randomforestclassifier__max_features': 7, 'randomforestclassifier__max_samples': 0.9, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 110, 'simpleimputer__strategy': 'median'}

# Best score for `model`: 0.8075296109752333
# Best params for `model`: {'randomforestclassifier__criterion': 'gini', 'randomforestclassifier__max_depth': 25, 'randomforestclassifier__max_features': 7, 'randomforestclassifier__max_samples': 0.9, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__n_estimators': 113, 'simpleimputer__strategy': 'median'}

"""# Communicate Results

**Task 9:** Create a DataFrame `submission` whose index is the same as `X_test` and that has one column `'status_group'` with your predictions. Next, save this DataFrame as a CSV file and upload your submissions to our competition site. 

**Note:** Check the `sample_submission.csv` file on the competition website to make sure your submissions follows the same formatting.
"""

clf_final = make_pipeline(
    OrdinalEncoder(),
    SimpleImputer(strategy=best_params['simpleimputer__strategy']),
    RandomForestClassifier(
        random_state=42,
        n_jobs=-1,
        criterion=best_params['randomforestclassifier__criterion'],
        max_depth=best_params['randomforestclassifier__max_depth'],
        min_samples_leaf=best_params['randomforestclassifier__min_samples_leaf'],
        n_estimators=best_params['randomforestclassifier__n_estimators']
    )
)

X_test.drop(columns=['waterpoint_type_group'], inplace=True)

clf_final.fit(X, y)

y_pred = clf_final.predict(X_test)
submission = pd.DataFrame({'status_group':y_pred}, index=X_test.index)
datestamp = pd.Timestamp.now().strftime('%Y-%m-%d_%H_%M_')
submission.to_csv(f'02 Unit 2/files/{datestamp}submission.csv')

