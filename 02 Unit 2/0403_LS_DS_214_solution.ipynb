{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0403 LS_DS_214_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKX1lBXh48mo"
      },
      "source": [
        "Lambda School Data Science\n",
        "\n",
        "*Unit 2, Sprint 1, Module 4*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmOtIVw-48mq"
      },
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'\n",
        "\n",
        "# Libraries for today's project\n",
        "from category_encoders import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfPzclWrxg3a"
      },
      "source": [
        "def green():\n",
        "  print('\\033[38;2;0;180;0m', end='')\n",
        "\n",
        "def color_percent(floaty):\n",
        "  if floaty >= .5:\n",
        "    green = 200\n",
        "    red = int(500 * (1 - floaty))\n",
        "  else:\n",
        "    red = 255\n",
        "    green = int(400 * floaty)\n",
        "\n",
        "  print(f'\\033[38;2;{red};{green};0m', end='')"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBU3e6fD48mq"
      },
      "source": [
        "# Module Project: Logistic Regression\n",
        "\n",
        "Do you like burritos? ðŸŒ¯ You're in luck then, because in this project you'll create a model to predict whether a burrito is `'Great'`.\n",
        "\n",
        "The dataset for this assignment comes from [Scott Cole](https://srcole.github.io/100burritos/), a San Diego-based data scientist and burrito enthusiast. \n",
        "\n",
        "## Directions\n",
        "\n",
        "The tasks for this project are the following:\n",
        "\n",
        "- **Task 1:** Import `csv` file using `wrangle` function.\n",
        "- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function .\n",
        "- **Task 3:** Split data into feature matrix `X` and target vector `y`.\n",
        "- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.\n",
        "- **Task 5:** Establish the baseline accuracy score for your dataset.\n",
        "- **Task 6:** Build `model_logr` using a pipeline that includes three transfomers and `LogisticRegression` predictor. Train model on `X_train` and `X_test`.\n",
        "- **Task 7:** Calculate the training and test accuracy score for your model.\n",
        "- **Task 8:** Create a horizontal bar chart showing the 10 most influencial features for your  model. \n",
        "- **Task 9:** Demonstrate and explain the differences between `model_lr.predict()` and `model_lr.predict_proba()`.\n",
        "\n",
        "**Note** \n",
        "\n",
        "You should limit yourself to the following libraries:\n",
        "\n",
        "- `category_encoders`\n",
        "- `matplotlib`\n",
        "- `pandas`\n",
        "- `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3PqbG7248mr"
      },
      "source": [
        "# I. Wrangle Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L4alYT-48mr"
      },
      "source": [
        "def wrangle(filepath):\n",
        "    # Import w/ DateTimeIndex\n",
        "    df = pd.read_csv(filepath, parse_dates=['Date'],\n",
        "                     index_col='Date')\n",
        "    \n",
        "    # Drop unrated burritos\n",
        "    df.dropna(subset=['overall'], inplace=True)\n",
        "    \n",
        "    # Derive binary classification target:\n",
        "    # We define a 'Great' burrito as having an\n",
        "    # overall rating of 4 or higher, on a 5 point scale\n",
        "    df['Great'] = (df['overall'] >= 4).astype(int)\n",
        "    \n",
        "    # Drop high cardinality categoricals\n",
        "    df = df.drop(columns=['Notes', 'Location', 'Address', 'URL', 'Neighborhood'])\n",
        "    \n",
        "    # Drop columns to prevent \"leakage\"\n",
        "    df = df.drop(columns=['Rec', 'overall'])\n",
        "\n",
        "    df['Queso'] = df['Queso'].astype(object)\n",
        "\n",
        "    for column in df.columns:\n",
        "      if df[column].dtype == 'object' and df[column].nunique() < 6:\n",
        "        # print(column)\n",
        "        df.loc[(df[column] == 'x') | (df[column] == 'x') | (df[column] == 'Yes'), column] = 1\n",
        "        df.loc[df[column] != 1, column] = 0\n",
        "        df[column] = df[column].astype(int)\n",
        "\n",
        "    # 'california', 'asada', 'surf', and 'carnitas'\n",
        "    df.loc[df['Burrito'].str.strip() == 'California', 'california'] = 1\n",
        "    df.loc[df['Burrito'].str.strip() == 'Carne asada', 'asada'] = 1\n",
        "    df.loc[df['Burrito'].str.strip() == 'Surf & Turf', 'surf'] = 1\n",
        "    df.loc[df['Burrito'].str.strip() == 'Carnitas', 'carnitas'] = 1 \n",
        "\n",
        "    for col in ['california', 'asada', 'surf', 'carnitas']:\n",
        "      df.loc[df[col] != 1, col] = 0\n",
        "\n",
        "    df = df.drop('Burrito', axis=1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "filepath = DATA_PATH + 'burritos/burritos.csv'"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbj80ywE48ms"
      },
      "source": [
        "**Task 1:** Use the above `wrangle` function to import the `burritos.csv` file into a DataFrame named `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LBhzfcm48ms",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "224aadc1-169a-4061-fc1c-63e3c4b37b16"
      },
      "source": [
        "filepath = DATA_PATH + 'burritos/burritos.csv'\n",
        "df = wrangle(filepath)\n",
        "df.head()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Yelp</th>\n",
              "      <th>Google</th>\n",
              "      <th>Chips</th>\n",
              "      <th>Cost</th>\n",
              "      <th>Hunger</th>\n",
              "      <th>Mass (g)</th>\n",
              "      <th>Density (g/mL)</th>\n",
              "      <th>Length</th>\n",
              "      <th>Circum</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Tortilla</th>\n",
              "      <th>Temp</th>\n",
              "      <th>Meat</th>\n",
              "      <th>Fillings</th>\n",
              "      <th>Meat:filling</th>\n",
              "      <th>Uniformity</th>\n",
              "      <th>Salsa</th>\n",
              "      <th>Synergy</th>\n",
              "      <th>Wrap</th>\n",
              "      <th>Reviewer</th>\n",
              "      <th>Unreliable</th>\n",
              "      <th>NonSD</th>\n",
              "      <th>Beef</th>\n",
              "      <th>Pico</th>\n",
              "      <th>Guac</th>\n",
              "      <th>Cheese</th>\n",
              "      <th>Fries</th>\n",
              "      <th>Sour cream</th>\n",
              "      <th>Pork</th>\n",
              "      <th>Chicken</th>\n",
              "      <th>Shrimp</th>\n",
              "      <th>Fish</th>\n",
              "      <th>Rice</th>\n",
              "      <th>Beans</th>\n",
              "      <th>Lettuce</th>\n",
              "      <th>Tomato</th>\n",
              "      <th>Bell peper</th>\n",
              "      <th>Carrots</th>\n",
              "      <th>Cabbage</th>\n",
              "      <th>Sauce</th>\n",
              "      <th>Salsa.1</th>\n",
              "      <th>Cilantro</th>\n",
              "      <th>Onion</th>\n",
              "      <th>Taquito</th>\n",
              "      <th>Pineapple</th>\n",
              "      <th>Ham</th>\n",
              "      <th>Chile relleno</th>\n",
              "      <th>Nopales</th>\n",
              "      <th>Lobster</th>\n",
              "      <th>Queso</th>\n",
              "      <th>Egg</th>\n",
              "      <th>Mushroom</th>\n",
              "      <th>Bacon</th>\n",
              "      <th>Sushi</th>\n",
              "      <th>Avocado</th>\n",
              "      <th>Corn</th>\n",
              "      <th>Zucchini</th>\n",
              "      <th>Great</th>\n",
              "      <th>california</th>\n",
              "      <th>asada</th>\n",
              "      <th>surf</th>\n",
              "      <th>carnitas</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2016-01-18</th>\n",
              "      <td>3.5</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0</td>\n",
              "      <td>6.49</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Scott</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>3.5</td>\n",
              "      <td>3.3</td>\n",
              "      <td>0</td>\n",
              "      <td>5.45</td>\n",
              "      <td>3.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Scott</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>4.85</td>\n",
              "      <td>1.5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Emily</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-24</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>5.25</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Ricardo</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016-01-27</th>\n",
              "      <td>4.0</td>\n",
              "      <td>3.8</td>\n",
              "      <td>1</td>\n",
              "      <td>6.59</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.5</td>\n",
              "      <td>4.5</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Scott</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Yelp  Google  Chips  Cost  ...  california  asada  surf  carnitas\n",
              "Date                                   ...                                   \n",
              "2016-01-18   3.5     4.2      0  6.49  ...         1.0    0.0   0.0       0.0\n",
              "2016-01-24   3.5     3.3      0  5.45  ...         1.0    0.0   0.0       0.0\n",
              "2016-01-24   NaN     NaN      0  4.85  ...         0.0    0.0   0.0       1.0\n",
              "2016-01-24   NaN     NaN      0  5.25  ...         0.0    1.0   0.0       0.0\n",
              "2016-01-27   4.0     3.8      1  6.59  ...         1.0    0.0   0.0       0.0\n",
              "\n",
              "[5 rows x 62 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI-Epa4Rp2rH",
        "outputId": "0d4291cd-85aa-4653-b16b-49909bb3744c"
      },
      "source": [
        "# print(df['Burrito'].value_counts())\n",
        "df['california'].value_counts()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0    294\n",
              "1.0    127\n",
              "Name: california, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIKzzO77n6dF"
      },
      "source": [
        "# df.info()"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTyX5nSC48mt"
      },
      "source": [
        "During your exploratory data analysis, note that there are several columns whose data type is `object` but that seem to be a binary encoding. For example, `df['Beef'].head()` returns:\n",
        "\n",
        "```\n",
        "0      x\n",
        "1      x\n",
        "2    NaN\n",
        "3      x\n",
        "4      x\n",
        "Name: Beef, dtype: object\n",
        "```\n",
        "\n",
        "**Task 2:** Change the `wrangle` function so that these columns are properly encoded as `0` and `1`s. Be sure your code handles upper- and lowercase `X`s, and `NaN`s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvzFw0gC48mt"
      },
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wULzS4Mo48mu"
      },
      "source": [
        "If you explore the `'Burrito'` column of `df`, you'll notice that it's a high-cardinality categorical feature. You'll also notice that there's a lot of overlap between the categories. \n",
        "\n",
        "**Stretch Goal:** Change the `wrangle` function above so that it engineers four new features: `'california'`, `'asada'`, `'surf'`, and `'carnitas'`. Each row should have a `1` or `0` based on the text information in the `'Burrito'` column. For example, here's how the first 5 rows of the dataset would look.\n",
        "\n",
        "| **Burrito** | **california** | **asada** | **surf** | **carnitas** |\n",
        "| :---------- | :------------: | :-------: | :------: | :----------: |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "|  Carnitas   |       0        |     0     |    0     |      1       |\n",
        "| Carne asada |       0        |     1     |    0     |      0       |\n",
        "| California  |       1        |     0     |    0     |      0       |\n",
        "\n",
        "**Note:** Be sure to also drop the `'Burrito'` once you've engineered your new features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iomTrVR348mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8339f1bb-9acf-47f3-d3d9-35f15529c274"
      },
      "source": [
        "# Conduct your exploratory data analysis here\n",
        "# And modify the `wrangle` function above.\n",
        "print('\\033[38;2;255;0;0mThis data wrangling stuff will be the end of me')"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;2;255;0;0mThis data wrangling stuff will be the end of me\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gVk4SQf48mv"
      },
      "source": [
        "# II. Split Data\n",
        "\n",
        "**Task 3:** Split your dataset into the feature matrix `X` and the target vector `y`. You want to predict `'Great'`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5YGFOkP48mv"
      },
      "source": [
        "target = 'Great'\n",
        "X = df.drop(columns=target)\n",
        "y = df[target]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-77EsBM48mv"
      },
      "source": [
        "**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).\n",
        "\n",
        "- Your training set should include data from 2016 through 2017. \n",
        "- Your test set should include data from 2018 and later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHDadJxM48mw"
      },
      "source": [
        "cutoff = '2018-01-01'\n",
        "mask = X.index < cutoff\n",
        "X_train, y_train = X.loc[mask], y.loc[mask]\n",
        "X_test, y_test = X.loc[~mask], y.loc[~mask]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GumKHery48mw"
      },
      "source": [
        "# III. Establish Baseline\n",
        "\n",
        "**Task 5:** Since this is a **classification** problem, you should establish a baseline accuracy score. Figure out what is the majority class in `y_train` and what percentage of your training observations it represents. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NUe1tui48mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2dfb27-de72-4409-acb5-04f903c258a0"
      },
      "source": [
        "baseline_acc = y_train.value_counts(normalize=True).max()\n",
        "color_percent(baseline_acc)\n",
        "\n",
        "print('Baseline Accuracy Score:', baseline_acc)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;2;208;200;0mBaseline Accuracy Score: 0.5822454308093995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OjzMwuh48mw"
      },
      "source": [
        "# IV. Build Model\n",
        "\n",
        "**Task 6:** Build a `Pipeline` named `model_logr`, and fit it to your training data. Your pipeline should include:\n",
        "\n",
        "- a `OneHotEncoder` transformer for categorical features, \n",
        "- a `SimpleImputer` transformer to deal with missing values, \n",
        "- a [`StandarScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) transfomer (which often improves performance in a logistic regression model), and \n",
        "- a `LogisticRegression` predictor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO2fAe7u48mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57026ddd-ef0b-447d-b7a8-cc7b40d6212e"
      },
      "source": [
        "model_logr = make_pipeline(\n",
        "    OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(strategy='mean'),\n",
        "    StandardScaler(),\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "model_logr.fit(X_train, y_train)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/category_encoders/utils.py:21: FutureWarning: is_categorical is deprecated and will be removed in a future version.  Use is_categorical_dtype instead\n",
            "  elif pd.api.types.is_categorical(cols):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('onehotencoder',\n",
              "                 OneHotEncoder(cols=['Reviewer'], drop_invariant=False,\n",
              "                               handle_missing='value', handle_unknown='value',\n",
              "                               return_df=True, use_cat_names=True, verbose=0)),\n",
              "                ('simpleimputer',\n",
              "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
              "                               missing_values=nan, strategy='mean',\n",
              "                               verbose=0)),\n",
              "                ('standardscaler',\n",
              "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
              "                ('logisticregression',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVinENBd48mx"
      },
      "source": [
        "# IV. Check Metrics\n",
        "\n",
        "**Task 7:** Calculate the training and test accuracy score for `model_lr`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmWnUDde48mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a870eff-ba16-4bb8-ecaf-3310a33fb502"
      },
      "source": [
        "training_acc = model_logr.score(X_train, y_train)\n",
        "test_acc = model_logr.score(X_test, y_test)\n",
        "\n",
        "color_percent(training_acc)\n",
        "print('Training MAE:', training_acc)\n",
        "\n",
        "color_percent(test_acc)\n",
        "print('Test MAE:', test_acc)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;2;15;200;0mTraining MAE: 0.9686684073107049\n",
            "\u001b[38;2;92;200;0mTest MAE: 0.8157894736842105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uwGUFDZ48mx"
      },
      "source": [
        "# V. Communicate Results\n",
        "\n",
        "**Task 8:** Create a horizontal barchart that plots the 10 most important coefficients for `model_lr`, sorted by absolute value.\n",
        "\n",
        "**Note:** Since you created your model using a `Pipeline`, you'll need to use the [`named_steps`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) attribute to access the coefficients in your `LogisticRegression` predictor. Be sure to look at the shape of the coefficients array before you combine it with the feature names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N1NNhBU48my",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "46895a57-999a-4318-a537-a70ea806d066"
      },
      "source": [
        "# Create your horizontal barchart here.\n",
        "coefs = model_logr.named_steps['logisticregression'].coef_[0]\n",
        "features = model_logr.named_steps['onehotencoder'].get_feature_names()\n",
        "feat_imp = pd.Series(coefs, index=features).sort_values(key=abs)\n",
        "feat_imp.tail(10).plot(kind='barh')\n",
        "plt.show()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O6dhmWC48my"
      },
      "source": [
        "There is more than one way to generate predictions with `model_lr`. For instance, you can use [`predict`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression) or [`predict_proba`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logisticregression#sklearn.linear_model.LogisticRegression.predict_proba).\n",
        "\n",
        "**Task 9:** Generate predictions for `X_test` using both `predict` and `predict_proba`. Then below, write a summary of the differences in the output for these two methods. You should answer the following questions:\n",
        "\n",
        "- What data type do `predict` and `predict_proba` output?\n",
        "- What are the shapes of their different output?\n",
        "- What numerical values are in the output?\n",
        "- What do those numerical values represent?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwei5P7W48my",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e104be-35e9-4a8f-b685-c23a2238254f"
      },
      "source": [
        "# Write code here to explore the differences between `predict` and `predict_proba`.\n",
        "y_pred = model_logr.predict(X_test)\n",
        "print(y_pred.shape)\n",
        "model_logr.predict_proba(X_test).shape"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(38,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(38, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz9_mXv748my"
      },
      "source": [
        "**Give your written answer here:**\n",
        "\n",
        "```\n",
        "predict and predict_proba both output arrays\n",
        "\n",
        "both have the same length, however proba is in pairs where predict is not\n",
        "\n",
        "predict shows the prediction (0 or 1), and predict_proba showing the likelihood (normalized)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPKY8z7M4876"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}